{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, CategoricalNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split # for train-test split \n",
    "from sklearn.preprocessing import StandardScaler # for feature scaling\n",
    "from sklearn.model_selection import GridSearchCV # for fine-tuning\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_roc_curve # for evaluation\n",
    "from sklearn.pipeline import make_pipeline # for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Generator\n",
    "from scipy import stats # for sampling\n",
    "from fitter import Fitter # for fitting the best distribution\n",
    "import copy # for copying nested dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # for visualization \n",
    "import seaborn as sns  # for coloring \n",
    "\n",
    "# set style of graphs\n",
    "plt.style.use('seaborn')\n",
    "from pylab import rcParams\n",
    "plt.rcParams['figure.dpi'] = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../dataset/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(train_data.columns)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœTo predict win/loss of a game, we can use one of the two ways:\n",
    "\n",
    "#1. Select only one feature (points), the win/loss prediction is just based on which team has the higher point.\n",
    "#2. Select features other than points, the win/loss is then based on the prediction of a classifier which takes those features as inputs.\n",
    "\n",
    "# In this notebook, we will use option (2) as it offers better range of uncertainty for simulation.\n",
    "\n",
    "selected_features = [\n",
    "    'FG_PCT_home', 'FT_PCT_home', 'FG3_PCT_home', 'AST_home', 'REB_home',\n",
    "    'FG_PCT_away', 'FT_PCT_away', 'FG3_PCT_away', 'AST_away', 'REB_away',\n",
    "    ]\n",
    "\n",
    "# check the features we selected\n",
    "X = train_data[selected_features]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data['HOME_TEAM_WINS']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting 70% training data, 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"X shape\", X_train.shape, \"y shape\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # print(f\"accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    # print(f\"AUC: {roc_auc_score(y_test, y_pred)}\")\n",
    "    # print(f\"log_loss: {log_loss(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for SVM a scaler it can improve accuracy of the model\n",
    "\n",
    "# scaler = StandardScaler() # initialize an instance \n",
    "# X_train_svm = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# train SVM\n",
    "\n",
    "support_vector_default = SVC() # initialize a model\n",
    "support_vector_default.fit(X_train, y_train) # fit(train) it with the training data and targets\n",
    "\n",
    "# check test score \n",
    "y_pred_svm_default = support_vector_default.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# fine-tuning hyperparameters\n",
    "#param_grid_svm = {'C': [0.1, 1, 10],\n",
    "#              'gamma': [1, 0.5, 0.1, 0.01, 0.001, 0.0001],\n",
    "#              'kernel': ['linear', 'poly', 'sigmoid', 'rbf']\n",
    "#            }\n",
    "param_grid_svm = {'C': [0.1],\n",
    "              'gamma': [0.1],\n",
    "              'kernel': ['linear', 'poly', 'rbf']\n",
    "            }\n",
    "\n",
    "grid_search_svm = GridSearchCV(estimator=support_vector_default, param_grid=param_grid_svm, cv=10, verbose=2, scoring='accuracy', \n",
    "                            n_jobs = -1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_svm.best_params_\n",
    "\n",
    "# {'C': 0.1, 'gamma': 1, 'kernel': 'linear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# train SVM\n",
    "\n",
    "support_vector = SVC(C=0.1, gamma=0.1, kernel='linear') # initialize a model\n",
    "support_vector.fit(X_train, y_train) # fit(train) it with the training data and targets\n",
    "\n",
    "# check test score \n",
    "y_pred_svm = support_vector.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Classifier\n",
    "random_forest_default = RandomForestClassifier()\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "random_forest_default.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rf_default = random_forest_default.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another round\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "# Create a base model\n",
    "grid_search_rf = GridSearchCV(estimator=random_forest_default, param_grid=param_grid_rf, cv=10, verbose=2, scoring='accuracy', \n",
    "                            n_jobs = -1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_rf.best_params_\n",
    "\n",
    "# {'bootstrap = True', max_depth = 30, max_features = 'auto', min_samples_leaf = 2, min_samples_split = 2, n_estimators = 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Classifier\n",
    "# random_forest = RandomForestClassifier(n_estimators=400, min_samples_split=2, min_samples_leaf=2, max_features='auto', max_depth=30, bootstrap='True')\n",
    "random_forest = RandomForestClassifier(n_estimators=500, max_features='sqrt', max_depth=50, bootstrap='True')\n",
    "# clf = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "random_forest.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rf = random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_nb_default = GaussianNB()\n",
    "gauss_nb_default.fit(X_train, y_train)\n",
    "y_pred_gauss_nb_default = gauss_nb_default.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_nb = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_nb = GridSearchCV(estimator=gauss_nb_default, param_grid=param_grid_nb, cv=10, verbose=2, scoring='accuracy', \n",
    "                            n_jobs = -1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_nb = GaussianNB(var_smoothing=1.873817422860383e-07)\n",
    "gauss_nb.fit(X_train, y_train)\n",
    "y_pred_gauss_nb = gauss_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting AUC for final hyperparametrizes classifiers\n",
    "\n",
    "fig = plot_roc_curve(support_vector, X_test, y_test, linewidth=5, linestyle = '-')\n",
    "fig = plot_roc_curve(gauss_nb, X_test, y_test, ax = fig.ax_, linewidth=5, linestyle = ':')\n",
    "fig = plot_roc_curve(random_forest, X_test, y_test, ax = fig.ax_, linewidth=5, linestyle = '--')\n",
    "\n",
    "plt.tick_params(axis='x', labelsize=24)\n",
    "plt.tick_params(axis='y', labelsize=24)\n",
    "plt.xlabel('False Positive Rate', fontsize=24)\n",
    "plt.ylabel('True Positive Rate', fontsize=24)\n",
    "\n",
    "plt.legend(fontsize=24) # using a size in points\n",
    "plt.legend(fontsize=\"xx-large\") # using a named size\n",
    "\n",
    "plt.savefig('../report/plots/auc_optimize.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting AUC for standard classifier\n",
    "\n",
    "fig = plot_roc_curve(support_vector_default, X_test, y_test, linewidth=3.5, linestyle = '-')\n",
    "fig = plot_roc_curve(gauss_nb_default, X_test, y_test, ax = fig.ax_, linewidth=3.5, linestyle = ':')\n",
    "fig = plot_roc_curve(random_forest_default, X_test, y_test, ax = fig.ax_, linewidth=3.5, linestyle = '--')\n",
    "\n",
    "plt.tick_params(axis='x', labelsize=24)\n",
    "plt.tick_params(axis='y', labelsize=24)\n",
    "plt.xlabel('False Positive Rate', fontsize=24)\n",
    "plt.ylabel('True Positive Rate', fontsize=24)\n",
    "\n",
    "plt.legend(fontsize=24) # using a size in points\n",
    "plt.legend(fontsize=\"xx-large\") # using a named size\n",
    "\n",
    "plt.savefig('../report/plots/auc_default.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics evaluation for standard classifier\n",
    "\n",
    "evaluate(y_test, y_pred_rf_default)\n",
    "evaluate(y_test, y_pred_svm_default)\n",
    "evaluate(y_test, y_pred_gauss_nb_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics evaluation for hyperparametrize classifier\n",
    "\n",
    "evaluate(y_test, y_pred_rf)\n",
    "evaluate(y_test, y_pred_svm)\n",
    "evaluate(y_test, y_pred_gauss_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fitting a Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like before, we had held out data from 2020-2021 playoff for real testing\n",
    "# Though large data is essential for fitting, for time-series problems, we give priority to the recent data most reflective of team's recent ability.\n",
    "# Since we aim to predict 2021-2022 playoff, here we will just fit the data from that regular session which starts in Oct, 2020.\n",
    "\n",
    "df_ = train_data.loc[train_data['SEASON'] > 2019].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_distributions = [\n",
    "    'norm','t', 'f', 'chi', 'cosine', 'alpha', \n",
    "    'beta', 'gamma', 'dgamma', 'dweibull',\n",
    "    'maxwell', 'pareto', 'fisk'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_teams = train_data['HOME_TEAM_ID'].unique() # extract all the unique teams\n",
    "\n",
    "# Since we don't care about whether the team was a host or visitor in each game, \n",
    "# we can just combine the features for all games.\n",
    "\n",
    "# Get all the data for teams\n",
    "all_team_sim_data = {}\n",
    "\n",
    "for team_name in unique_teams:\n",
    "    \n",
    "    # find games where the team is either the host or guest\n",
    "    df_team = df_.loc[(df_['HOME_TEAM_ID'] == team_name) | (df_['VISITOR_TEAM_ID'] == team_name)]\n",
    "    # it is home team, select the first 5 features\n",
    "    df_1 = df_team.loc[df_team['HOME_TEAM_ID'] == team_name][selected_features[:5]]\n",
    "    # it is guest team, select the first 5 features\n",
    "    df_0 = df_team.loc[df_team['VISITOR_TEAM_ID'] == team_name][selected_features[5:]]\n",
    "\n",
    "    # combine them\n",
    "    df_0.columns = df_1.columns # before concating, match the column names\n",
    "    df_s = pd.concat([df_1, df_0], axis = 0)\n",
    "    \n",
    "    # convert the pandas.DataFrame to numpy array\n",
    "    all_team_sim_data[team_name] = df_s.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_team_sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data format:\n",
    "#   team_name => list of feature distributions => dictionary with distribution name and parameters\n",
    "#   e.g.,\n",
    "#   megadata = {\n",
    "      #'Timberwolves': [{'beta': (0.23, 0.3, 0.3, 0.4)}, {'nor': (0.23, 0.3,)}, ..], \n",
    "      #'Warriors':[{}, {},...]\n",
    "      #  }\n",
    "    \n",
    "megadata = {} # store the data that our Generator will rely on\n",
    "for team_name in unique_teams:\n",
    "    \n",
    "    feature_dis_paras = []\n",
    "    data = all_team_sim_data[team_name]\n",
    "    \n",
    "    # 5 features for each team\n",
    "    for i in range(5): \n",
    "        f = Fitter(data[:, i]) # initalize a Fitter instance\n",
    "        f.distributions = selected_distributions # use only the selected distributions (faster)\n",
    "        f.fit() # do the fitting \n",
    "        best_paras = f.get_best(method='sumsquare_error') # get the best fitted paras\n",
    "        feature_dis_paras.append(best_paras)\n",
    "        \n",
    "    megadata[team_name] = feature_dis_paras\n",
    "    \n",
    "# print('Features for all teams have been fitted!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = megadata.copy() # data that Generator must rely on\n",
    "\n",
    "GEN = {\n",
    " 'alpha': stats.alpha.rvs,\n",
    " 'beta': stats.beta.rvs,\n",
    " 'chi': stats.chi.rvs,\n",
    " 'cosine': stats.cosine.rvs,\n",
    " 'dgamma': stats.dgamma.rvs,\n",
    " 'dweibull':stats.dweibull.rvs,\n",
    " 'f':stats.f.rvs,\n",
    " 'fisk':stats.fisk.rvs,\n",
    " 'gamma': stats.gamma.rvs,\n",
    " 'maxwell':stats.maxwell.rvs,\n",
    " 'norm':stats.norm.rvs,\n",
    " 'pareto':stats.pareto.rvs,\n",
    " 't':stats.t.rvs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIS = make_pipeline(scaler, support_vector)\n",
    "# DIS = make_pipeline(random_forest)\n",
    "DIS = make_pipeline(gauss_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process: \n",
    "\n",
    "1. sampling: \"generate feature values used for making win/loss prediction\"\n",
    "2. predict: \"predict the win or loss of  n game(s) played by two tems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    A game between two teams:\n",
    "    \n",
    "    - feature values sampled from Generator\n",
    "    - win/loss predicted by Discriminator\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__ (self, random_state = None):\n",
    "        \n",
    "        self.random_state = random_state # keep this to None for making simulations \n",
    "    \n",
    "    def predict(self, team1, team2, num_games = 1):\n",
    "        \n",
    "        '''Taking as input two teams predict the win or loss of  n game(s)'''\n",
    "        \n",
    "        assert num_games >= 1, \"at least one game must be played\"\n",
    "        # output numpy array\n",
    "        team_1_feature_data = DATA[team1] # take generator data from team 1\n",
    "        team_2_feature_data = DATA[team2] # take generator data from team 2\n",
    "        \n",
    "        features = []\n",
    "        for feature_paras_1 in team_1_feature_data:\n",
    "            # sampling 5 features for team 1\n",
    "            sample_1 = self.sampling(feature_paras_1, size = num_games) \n",
    "            features.append(sample_1) \n",
    "            \n",
    "        for feature_paras_2 in team_2_feature_data:\n",
    "            # sampling 5 features for team 1\n",
    "            sample_2 = self.sampling(feature_paras_2, size = num_games) \n",
    "            features.append(sample_2)\n",
    "\n",
    "        features = np.array(features).T # collect together the features of the two teams \n",
    "                                        # to simulate a possible match\n",
    "\n",
    "        win_loss = DIS.predict(features) # use the classifier to predict the result of this hypotethical match\n",
    "        \n",
    "        return list(win_loss) # a list of win/loss from num_games\n",
    "    \n",
    "    \n",
    "    def sampling(self, dic, size = 1, random_state = None):\n",
    "        \n",
    "        '''generate feature values used for making win/loss prediction'''\n",
    "                        \n",
    "        dis_name = list(dic.keys())[0] # get the distribution type\n",
    "        paras = list(dic.values())[0] # get the distribution parameters\n",
    "    \n",
    "        # create the random sample from the distribution\n",
    "        sample = GEN[dis_name](*paras, size = size,  random_state =  random_state)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalTournament(Game):\n",
    "    \n",
    "    ''' Best-of-7 elimination, 16 teams, 4 rounds in total to win championship '''\n",
    "    \n",
    "    def __init__(self, n_games_per_group = 7, winning_threshold = 4, random_state = None):\n",
    "\n",
    "        self.n_games_per_group  = n_games_per_group\n",
    "        self.winning_threshold = winning_threshold\n",
    "        self.team_list = None\n",
    "        self.rounds = {} # keep track the number of times a team wins at each round \n",
    "        super().__init__(random_state)\n",
    "        \n",
    "    \n",
    "    def simulate(self, group_list, n_simulation = 1, probs = True):\n",
    "        \n",
    "        ''' simulate the entire playoff n times and also record the accumulated wins'''\n",
    "             \n",
    "        # update the list of teams\n",
    "        self.rounds = {}\n",
    "        self.team_list = [i[0] for i in group_list] + [i[1] for i in group_list]\n",
    "        \n",
    "        for i in range(n_simulation):\n",
    "            # print(f\"epoch number: {i}\")\n",
    "            cham = self.one_time_simu(group_list)\n",
    "        if probs:\n",
    "            self.rounds_probs = self._compute_probs()\n",
    "            \n",
    "    \n",
    "    def one_time_simu(self, group_list, verbose = False, probs = False):\n",
    "        \n",
    "        ''' simulate the entire playoff once and also record the accumulated wins'''\n",
    "        \n",
    "        # update the list of teams if haven't done so\n",
    "        if self.team_list == None: \n",
    "            self.team_list = [i[0] for i in group_list] + [i[1] for i in group_list]\n",
    "        round_number, done = 0, 0\n",
    "        while not done: \n",
    "            all_group_winners, group_list = self.play_round(group_list)\n",
    "            # retrive round stats\n",
    "            try:\n",
    "                updated_round_stats = self.rounds[round_number]\n",
    "            except KeyError:\n",
    "                updated_round_stats = {}\n",
    "                for team in self.team_list:\n",
    "                    updated_round_stats[team] = 0\n",
    "            # if a team wins, record + 1 \n",
    "            for winner in all_group_winners:\n",
    "                try: \n",
    "                    updated_round_stats[winner] += 1\n",
    "                except KeyError:\n",
    "                    pass     \n",
    "            self.rounds[round_number] = updated_round_stats\n",
    "            if verbose:\n",
    "                print('{} round played'.format(round_number))\n",
    "            if probs:\n",
    "                self.rounds_probs = self._compute_probs()\n",
    "            if type(group_list) != list: # if it becomes the final\n",
    "                done = 1\n",
    "            round_number += 1\n",
    "            \n",
    "        return group_list\n",
    "\n",
    "        \n",
    "    def play_round(self, group_list):\n",
    "        \n",
    "        '''play a round of games based of a list of paired teams'''\n",
    "        \n",
    "        all_group_winners = [] \n",
    "        # play each group and get the group winner\n",
    "        for group in group_list:\n",
    "            winner = self.play_n_games(group[0], group[1])\n",
    "            all_group_winners.append(winner)\n",
    "        \n",
    "        if len(all_group_winners) > 1:\n",
    "            new_group_list = []         \n",
    "            for index in range(0, len(all_group_winners), 2):\n",
    "                # first winner, second winner\n",
    "                new_group = [all_group_winners[index], all_group_winners[index + 1]]\n",
    "                new_group_list.append(new_group)\n",
    "                \n",
    "            return all_group_winners, new_group_list\n",
    "        else:  \n",
    "            return all_group_winners, winner\n",
    "        \n",
    "        \n",
    "    def play_n_games(self, team1, team2):\n",
    "        \n",
    "        \n",
    "        '''simulate data, and then use our classifier to predict win/loss'''\n",
    "        result = Game().predict(team1, team2, self.n_games_per_group)\n",
    "        if sum(result[:4]) == self.winning_threshold or sum(result) >= self.winning_threshold:\n",
    "            winner = team1 # home team wins\n",
    "        else:\n",
    "            winner = team2 # visitor team wins\n",
    "            \n",
    "        return winner\n",
    "    \n",
    "    def rounds_list(self):\n",
    "        list_round = self.rounds\n",
    "        return list\n",
    "    \n",
    "    def _compute_probs(self):\n",
    "        \n",
    "        '''prob = wins for a team / sum of wins for all teams at a particular round'''\n",
    "        \n",
    "        rounds_probs = copy.deepcopy(self.rounds)\n",
    "        for round_number, round_stats in rounds_probs.items():\n",
    "            m = np.sum(list(round_stats.values()))\n",
    "            for k, v in rounds_probs[round_number].items():\n",
    "                rounds_probs[round_number][k] = v / m\n",
    "                \n",
    "        return rounds_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2022\n",
    "group_list_2022 = [\n",
    "     # Eastern Conference\n",
    "     ('Heat', 'Hawks'),  # group A 1 \n",
    "     ('76ers', 'Raptors'), # group B 4 \n",
    "    \n",
    "     ('Bucks', 'Bulls'), # group C 3 \n",
    "     ('Celtics', 'Nets'), # group D 2\n",
    "    \n",
    "     # Western Conference\n",
    "     ('Suns','Pelicans'),  # group E 1 \n",
    "     ('Mavericks','Jazz'), # group F 4 \n",
    "    \n",
    "     ('Warriors', 'Nuggets'), # group G 3 \n",
    "     ('Grizzlies', 'Timberwolves')] # group H 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initiate a playoff\n",
    "playoff = FinalTournament()\n",
    "\n",
    "# simulate the playoff 5,000 times\n",
    "\n",
    "playoff.simulate(group_list_2022, n_simulation = 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playoff.rounds_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(rounds_data):\n",
    "    \n",
    "    rounds_stats = list(rounds_data.values())\n",
    "    team_names = list(rounds_stats[0].keys())\n",
    "    \n",
    "    # x is number of rounds used for labels, y is a 2-D array of (n_teams, n_rounds) used for data\n",
    "    x = list(rounds_data.keys())\n",
    "    y = np.array([list(r.values()) for r in rounds_stats]).T \n",
    "    \n",
    "    # we need at least 16 different colors, one for each team\n",
    "    c_1 =  sns.color_palette('tab10', n_colors = 10)\n",
    "    c_2 =  sns.color_palette(\"pastel\", n_colors = 10)\n",
    "    color_map = c_1 + c_2 \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.stackplot(x, y, labels = team_names, colors = color_map) \n",
    "    plt.legend(bbox_to_anchor=(1.1, 1.1), loc = 'upper left', fontsize=13)\n",
    "    plt.xticks(x, fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.xlabel('Round Number', fontsize = 15)\n",
    "    plt.title('Winning probabilities by all Teams & Rounds', pad = 20, fontsize = 24)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that a team's wins should get less and less in later rounds\n",
    "fig = plotting(playoff.rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results: probabilities of winning for all teams at each round\n",
    "fig = plotting(playoff.rounds_probs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0547785ecde31d30de7bdd91c2f6f8cc2815ed6bf5dfef2121b202953251c35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
